{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Depression detection on social media",
      "provenance": [],
      "collapsed_sections": [
        "0UxrFVR8w67r",
        "Nly-uFubNhtq",
        "eqetMK2Pe8k-",
        "uNSSmzurfwvq",
        "JbJMHvbWf2C8"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWTvhK2+0K1nHEYZQqAXgR"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UxrFVR8w67r"
      },
      "source": [
        "# Dataset Prep\n",
        "\n",
        "Following section will be on data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-i7BV9MuCmV"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNx0P6Dr5unj"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5Eh-zUxJz0"
      },
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/Allenfp/DepressionDetectionNLP/master/final_training_data.txt', sep='\\n')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS0BSDsk5_86"
      },
      "source": [
        "suicide_phrases = ['sw']\n",
        "casual_phrases = ['cc']\n",
        "suicide_pattern = [nlp(text) for text in suicide_phrases]\n",
        "casual_pattern = [nlp(text) for text in casual_phrases]\n",
        "matcher.add('SuicideWatch', None, *suicide_pattern)\n",
        "matcher.add('CasualConversation', None, *casual_pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CkshV2wxmTp"
      },
      "source": [
        "labels = []\n",
        "\n",
        "for text in train_df:\n",
        "  sent = train_df[text]\n",
        "  for idx in range(len(sent)):\n",
        "    label = sent[idx][-3:]\n",
        "    labels.append(label[-2:])\n",
        "    sent[idx] = sent[idx][:-3]\n",
        "\n",
        "train_df['Labels'] = labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Op96W59r9a"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRNAwJpUR8LF"
      },
      "source": [
        "train_df['Labels'].replace({\n",
        "    'sw': 1,\n",
        "    'cc': 0\n",
        "}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nly-uFubNhtq"
      },
      "source": [
        "# Data Format Prep\n",
        "\n",
        "Change data format to .tsv for BERT feeding, as BERT is more familiar to this format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdfsB1sCP4fj"
      },
      "source": [
        "train_df_bert = pd.DataFrame({\n",
        "    'id': range(len(train_df)),\n",
        "    'label': train_df['Labels'],\n",
        "    'alpha': ['a']*train_df.shape[0],\n",
        "    'text': train_df['combined\\tsubreddit'].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "train_df_bert.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr2xuPzFSdRI"
      },
      "source": [
        "train_df_bert.to_csv('train.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw0czzyuQij6"
      },
      "source": [
        "# Env Prep\n",
        "Setup the environment for BERT\n",
        "\n",
        "For this step, it is important to add GPU or TPU on the notebook by going through the toolbar in this particular order `Edit > Notebook Settings > Add accelerator > GPU`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG0QNtFzRBFC"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJt70KTRIQa"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVju7s1qR61h"
      },
      "source": [
        "# BERT Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AypUb0UG7qp"
      },
      "source": [
        "## Preparation for BERT\n",
        "\n",
        "This includes installing and importing some dependencies, connects to google drive, and to load locally located files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Wr87K2SAmI"
      },
      "source": [
        "# install\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp\n",
        "!pip install transformers\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2QAh-Da9Rx"
      },
      "source": [
        "**Use the following part to load summarized text from local drive.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* `train_summed.tsv`\n",
        ">  Summarize all texts to min length of 100 with ratio of 0.5\n",
        "\n",
        "\n",
        "* `train_summed(2).tsv`\n",
        "> Summarize long texts to min length of 20 with ratio of 0.5\n",
        "\n",
        "\n",
        "* `train_summed(4).tsv`\n",
        "> Summarize long texts to min length of 100 with ratio of 0.5 \n",
        ">> _Fails as this contains 13 text considered too lengthy and contains 83 NaN_\n",
        "\n",
        "\n",
        "* `train_summed(5).tsv`\n",
        "> Summarize long texts to min length of 20 with ratio of 0.4\n",
        ">> _Fails as this contains 14 NaN_\n",
        "\n",
        "\n",
        "* `train_summed(6).tsv`\n",
        "> Summarize long texts to min length of 20 with ratio of 0.3\n",
        ">> _Fails as this contains 14 NaN_\n",
        "\n",
        "\n",
        "* `train_summed(7).tsv`\n",
        "> Summarize long texts to min length of 20 with ratio of 0.3\n",
        ">> _Fails as this contains 14 NaN_\n",
        "\n",
        "\n",
        "* `train_summed(8).tsv`\n",
        "> Summarize long texts to min length of 20 with ratio of 0.1\n",
        "\n",
        "* `train_summed(9).tsv`\n",
        "> Summarize long texts to min length of 20 with ration of 0.9\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNDpsV1xS_NT"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRONtGXcTC8x"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVbLJit3BOkL"
      },
      "source": [
        "train_df_bert = pd.read_csv('train_summed(5).tsv', sep='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCsNe5hhQ4HZ"
      },
      "source": [
        "train_df_bert.rename(columns={0: 'id', 1: 'label', 2: 'alpha', 3: 'text'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S3DGcB9Bjmp"
      },
      "source": [
        "train_df_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLZ24IurimrJ"
      },
      "source": [
        "train_df_bert = train_df_bert.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqetMK2Pe8k-"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTSXlX0UdXyi"
      },
      "source": [
        "!pip install bert-extractive-summarizer\n",
        "!pip install spacy\n",
        "!pip install transformers==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r2rozQThVMd"
      },
      "source": [
        "from summarizer import Summarizer\n",
        "\n",
        "model = Summarizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qmKoHsKBoA_"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXg1s0R3hoIs"
      },
      "source": [
        "for idx in range(len(train_df_bert)):\n",
        "  print(idx)\n",
        "  if len(train_df_bert.loc[idx, 'text']) > 510:\n",
        "    print(f'Token count: {len(train_df_bert.loc[idx, \"text\"])}')\n",
        "    print(f\"Pre = {train_df_bert.loc[idx, 'text']}\")\n",
        "    result = model(train_df_bert.loc[idx, 'text'], ratio = 0.9)\n",
        "    train_df_bert.at[idx, 'text'] = result\n",
        "    print(f\"Post= {train_df_bert.loc[idx, 'text']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ3aj8Gn3enI"
      },
      "source": [
        "train_df_bert.to_csv('train_summed(9).tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLkQOMOoHKDe"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "This part includes tokenization using `BertTokenizer`\n",
        "\n",
        "Note that since some sentence in our dataset contains more than 512 tokens, we offer some options to handle this issue, those are:\n",
        "\n",
        "\n",
        "*   Stopwords Removal\n",
        "*   Slicing and Truncation of Texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEX4QThQFbzA"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWIY1FLKvN0R"
      },
      "source": [
        "train_df_bert = train_df_bert.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gt3Dw79Q-zo"
      },
      "source": [
        "labels = []\n",
        "for lbl in train_df_bert.label.values:\n",
        "  labels.append(lbl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEoBrxl-7mx"
      },
      "source": [
        "train_df_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK01zAwLHdM2"
      },
      "source": [
        "### Without Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcYbBbTwR53k"
      },
      "source": [
        "#Without stopwords removal\n",
        "\n",
        "input_ids = []\n",
        "lengths = []\n",
        "\n",
        "for (idx, sen) in enumerate(train_df_bert.text.values, start=0):\n",
        "  # print(sen)\n",
        "  encoded_sent = tokenizer.encode(\n",
        "      sen,\n",
        "      add_special_tokens = True\n",
        "      #max_length = 512\n",
        "  )\n",
        "\n",
        "  input_ids.append(encoded_sent)\n",
        "  lengths.append(len(encoded_sent))\n",
        "\n",
        "print(f'{len(input_ids):<10} comments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u9js1MBHuZN"
      },
      "source": [
        "## Text Slicing and Truncation\n",
        "\n",
        "According to the paper https://arxiv.org/pdf/1905.05583.pdf, there are 3 ways we can deal with long text for BERT.\n",
        "\n",
        "*Check section 5.3 in the paper for enclosure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODq8q541ftmL"
      },
      "source": [
        "### First 512 Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gM0IoKIfsKS"
      },
      "source": [
        "for id in input_ids:\n",
        "  if len(id) > 510:\n",
        "    head = id[:509]\n",
        "    tail = id[:-1]\n",
        "    id = head + tail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNSSmzurfwvq"
      },
      "source": [
        "### Combination of First and Last"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT2yYi8GfzUL"
      },
      "source": [
        "print(f'{len(input_ids):<10} comments')\n",
        "for id in input_ids:\n",
        "  if len(id) > 510:\n",
        "    head = id[:128]\n",
        "    tail = id[-382:]\n",
        "    id = head + tail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbJMHvbWf2C8"
      },
      "source": [
        "### Last 512 Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URgzEpG8hCDJ"
      },
      "source": [
        "for id in input_ids:\n",
        "  if len(id) > 510:\n",
        "    head = id[0]\n",
        "    tail = id[-509:]\n",
        "    id = head + tail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCr0NCK8KfKo"
      },
      "source": [
        "## Pad Texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivOPoVAIKiy3"
      },
      "source": [
        "MAX_LEN = 512\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFR8fzRtHn1_"
      },
      "source": [
        "## Verification Point\n",
        "\n",
        "The following codes' purpose is only to verify our data. This section focus only on checking whether the data is already compatible with the format that BERT prefers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v14YfsQATgXL"
      },
      "source": [
        "print(f'{np.sum(train_df_bert.label)} depressed')\n",
        "print(f'{len(train_df_bert.label) - np.sum(train_df_bert.label)} casual')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMg0EW1xT8mi"
      },
      "source": [
        "print(f'Min length: {min(lengths)} tokens')\n",
        "print(f'Max length: {max(lengths)} tokens')\n",
        "print(f'Med length: {np.median(lengths)} tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lYBlZolH1lp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "\n",
        "lengths = [min(l, 512) for l in lengths]\n",
        "sns.distplot(lengths, kde=False, rug=False)\n",
        "\n",
        "plt.title('Comment lengths')\n",
        "plt.xlabel('Comment length')\n",
        "plt.ylabel('# of comments')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AgXTcSnJJhg"
      },
      "source": [
        "num_truncated = lengths.count(512)\n",
        "\n",
        "num_sentences = len(lengths)\n",
        "prnct = float(num_truncated) / float(num_sentences)\n",
        "\n",
        "print(f'{num_truncated} of {num_sentences} sentences ({prnct:.1%}) are longer than 512 tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNR_hM_JJ5EK"
      },
      "source": [
        "num_sw = 0\n",
        "num_cc = 0\n",
        "\n",
        "for i, l in enumerate(lengths):\n",
        "  if l == 512:\n",
        "    if train_df_bert.label[l] == 1:\n",
        "      num_sw+=1\n",
        "    else:\n",
        "      num_cc+=1\n",
        "\n",
        "print(f'{num_sw} comments contains depressed text, the rest {num_cc} are OK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE8Y8YzGylFs"
      },
      "source": [
        "### Train Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yja6V2iZgWUa"
      },
      "source": [
        "## KFold\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "  att_mask = [int(token_id > 0) for token_id in sent]\n",
        "  attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8lQ0U1Sjh0"
      },
      "source": [
        "# BERT BERT BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMmOpAtCSmvb"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41sB5tGA6np"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuseWeSpS7OV"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "print(f'The BERT model has {len(params)} different named parameters.\\n')\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(f\"{p[0]:<55} {str(tuple(p[1].size())):>12}\")\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(f\"{p[0]:<55} {str(tuple(p[1].size())):>12}\")\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(f\"{p[0]:<55} {str(tuple(p[1].size())):>12}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbxGSFf_TcSU"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-XnvKIuUUfB"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 2\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1, num_training_steps=total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-X1UwOLU8p7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9gSA-PhU9_D"
      },
      "source": [
        "# Accuracy calculator\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqXY66XrVOSb"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# Time counter\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua0H06MQcJYv"
      },
      "source": [
        "## K-fold\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "epochs = 2\n",
        "seed_val = 42\n",
        "# accumulation_steps = 24\n",
        "batch_size = 8\n",
        "train_loss_set = []\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "training_stats = []\n",
        "kfold = KFold(n_splits=5)\n",
        "    \n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "# Perform one full pass over the training set.\n",
        "\n",
        "print(\"\")\n",
        "print('Training...')\n",
        "\n",
        "# Measure how long the training epoch takes.\n",
        "t0 = time.time()\n",
        "\n",
        "# Reset the total loss for this epoch.\n",
        "total_train_loss = 0\n",
        "total_train_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kfold.split(input_ids, attention_masks)):\n",
        "\n",
        "  ### Dividing data into folds\n",
        "  training_input = input_ids[train_index]\n",
        "  validation_input = input_ids[test_index]\n",
        "  training_labels = tuple(labels[j] for i,j in enumerate(train_index))\n",
        "  validation_labels = labels[test_index[0]:test_index[-1]+1]\n",
        "\n",
        "  # print(train_attention_masks)\n",
        "  # print(train_attention_masks[train_index[0]])\n",
        "\n",
        "  training_masks = tuple(attention_masks[j] for i,j in enumerate(train_index))\n",
        "  validation_masks = attention_masks[test_index[0]:test_index[-1]+1]\n",
        "\n",
        "  training_inputs = torch.tensor(training_input)\n",
        "  validation_inputs = torch.tensor(validation_input)\n",
        "  training_labels = torch.tensor(training_labels)\n",
        "  validation_labels = torch.tensor(validation_labels)\n",
        "  training_masks = torch.tensor(training_masks)\n",
        "  validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "\n",
        "  train_data = TensorDataset(training_inputs, training_masks, training_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_loader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  print('\\nFold number {} / {}'.format(fold + 1 , kfold.get_n_splits()))\n",
        "\n",
        "  # Put the model into training mode. Don't be mislead--the call to \n",
        "  # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "  # `dropout` and `batchnorm` layers behave differently during training\n",
        "  # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "  for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_loader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        output = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "        loss = output[0]\n",
        "        logits = output[1]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # if (step+1) % accumulation_steps == 0:        # Wait for several backward steps\n",
        "        print(f'Loss on step #{step}: {loss.item()}')\n",
        "        train_loss_set.append(loss.item())\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average accuracy over the training data.\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_train_loss / len(train_loader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch_i)\n",
        "    writer.add_scalar('Accuracy/train', avg_train_accuracy, epoch_i)\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_loader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        eval_loss += loss.item()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    avg_val_loss = eval_loss/nb_eval_steps\n",
        "    avg_val_accuracy = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    print(\"  Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    writer.add_scalar('Loss/val', avg_val_loss, epoch_i)\n",
        "    writer.add_scalar('Accuracy/val', avg_val_accuracy, epoch_i)\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Accur.': avg_train_accuracy,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4FrQ_PTB2eb"
      },
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiVQkGa1F2SN"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLqrPSpSGmdU"
      },
      "source": [
        "tensorboard --logdir runs --port=6007"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wwyXzL3GrtK"
      },
      "source": [
        "!kill 827"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ua1d_ODODu5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 6)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7mKfE261esf"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks2z6XfDZNDt"
      },
      "source": [
        "### Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lkbpkHyCWk"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR63HrWDZOii"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsZKxmTNGkq9"
      },
      "source": [
        "test_df_control = pd.read_csv('twitter_test(2).csv', sep='\\t', encoding='unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG4BsLIHHLdz"
      },
      "source": [
        "df_test = test_df_control.rename(columns = {'0': 'tweet'}, inplace = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cUpDb06ehTE"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df_test = shuffle(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFf7ZqgQZasW"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVGfCwpaZj54"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df_test.tweet.values\n",
        "labels = df_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWtpkfZZZnTs"
      },
      "source": [
        "#Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "test_accuracy = 0\n",
        "nb_test_steps = 0\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "   # Calculate the accuracy for this batch of test sentences.\n",
        "  tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
        "  \n",
        "  # Accumulate the total accuracy.\n",
        "  test_accuracy += tmp_test_accuracy\n",
        "\n",
        "  nb_test_steps += 1\n",
        "\n",
        "\n",
        "\n",
        "  # print(f'Temp: {tmp_test_accuracy}')\n",
        "  # print(f'Curr: {test_accuracy}')\n",
        "\n",
        "print(f'Accuracy: {test_accuracy/nb_test_steps}')\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGz9E3Xgxkn9"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceHPfig4w2DW"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USCY_HPIxH04"
      },
      "source": [
        "class_names = ['Control', 'Depressed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdhlZbrnwBfB"
      },
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions, target_names=class_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDYgRyF_tGY"
      },
      "source": [
        "# XLNet2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfqAt_tE_u_Q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-ggGtSO_yhk"
      },
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXM4ZJ1S_0J7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBN7OVv_1R7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDyBOcXu_2lG"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494wnacv_4wo"
      },
      "source": [
        "df = pd.read_csv(\"train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmsgETnzQy6c"
      },
      "source": [
        "train_df = df.iloc[:3000]\n",
        "test_df = df.iloc[3000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTpGxaih5Wdp"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_df = shuffle(train_df)\n",
        "test_df = shuffle(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSG7BEpg_-s0"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdahSK2b__-F"
      },
      "source": [
        "train_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyO7TRjpILC3"
      },
      "source": [
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7iY8h3AC5_"
      },
      "source": [
        "train_sentences = train_df.sentence.values\n",
        "test_sentences = test_df.sentence.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRoAirZaAFdF"
      },
      "source": [
        "train_sentences = [sentence + \" <SEP><CLS>\" for sentence in train_sentences]\n",
        "train_labels = train_df.label.values\n",
        "\n",
        "test_sentences = [sentence + \" <SEP><CLS>\" for sentence in test_sentences]\n",
        "test_labels = test_df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDP25Wm5ksCt"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hbat_gdAgm7"
      },
      "source": [
        "train_tokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
        "test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISpq2njwnCLR"
      },
      "source": [
        "train_tokenized_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn4esSZDAtqh"
      },
      "source": [
        "MAX_LEN = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG7KHSY4AwvA"
      },
      "source": [
        "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "train_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in train_tokenized_texts]\n",
        "test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_tokenized_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPoC0vXaAyzt"
      },
      "source": [
        "# Pad our input tokens\n",
        "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDhLibVFA1af"
      },
      "source": [
        "# Create attention masks\n",
        "train_attention_masks = []\n",
        "test_attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in train_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  train_attention_masks.append(seq_mask)\n",
        "\n",
        "print(len(train_attention_masks))\n",
        "for seq in test_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  test_attention_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ykSWrybA2no"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "training_inputs, validation_inputs, training_labels, validation_labels = train_test_split(train_input_ids, train_labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "training_masks, validation_masks, _, _ = train_test_split(train_attention_masks, train_input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO0LbcVjA44w"
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "training_inputs = torch.tensor(training_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "training_labels = torch.tensor(training_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "training_masks = torch.tensor(training_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJwcYLPCA-vv"
      },
      "source": [
        "# # Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
        "batch_size = 4\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(training_inputs, training_masks, training_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4PLLkhBAnH"
      },
      "source": [
        "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
        "\n",
        "from transformers import XLNetForSequenceClassification\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT5cIp4SBEAi"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RCauipuBGwr"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=5e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfcs0j798RCc"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# Time counter\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNF9n5SgBxe2"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHkx7O7DDqgY"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "total_acc = 0\n",
        "train_loss_set = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "epochs = 4\n",
        "batch_size = 8\n",
        "accumulation_steps = 24\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "kfold = KFold(n_splits=5)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kfold.split(train_input_ids, train_attention_masks)):\n",
        "\n",
        "    ### Dividing data into folds\n",
        "    training_input = train_input_ids[train_index]\n",
        "    validation_input = train_input_ids[test_index]\n",
        "    training_labels = train_labels[train_index]\n",
        "    validation_labels = train_labels[test_index]\n",
        "\n",
        "    # print(train_attention_masks)\n",
        "    # print(train_attention_masks[train_index[0]])\n",
        "\n",
        "    training_masks = tuple(train_attention_masks[j] for i,j in enumerate(train_index))\n",
        "    validation_masks = train_attention_masks[test_index[0]:test_index[-1]+1]\n",
        "\n",
        "    print(test_index[-1])\n",
        "    print(test_index[-1]+1)\n",
        "\n",
        "    training_inputs = torch.tensor(training_input)\n",
        "    validation_inputs = torch.tensor(validation_input)\n",
        "    training_labels = torch.tensor(training_labels)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "    training_masks = torch.tensor(training_masks)\n",
        "    validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(training_inputs, training_masks, training_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "    validation_loader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "    for _ in trange(epochs, desc='Epoch'):\n",
        "      t0 = time.time()\n",
        "      print('\\nFold number {} / {}'.format(fold + 1 , kfold.get_n_splits()))\n",
        "      model.train()\n",
        "      for step, x_batch in enumerate(train_loader):\n",
        "        # Add batch to GPU\n",
        "        x_batch = tuple(t.to(device) for t in x_batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = x_batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        train_loss_set.append(loss.item())    \n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        if (step+1) % accumulation_steps == 0:        # Wait for several backward steps\n",
        "          optimizer.step()                            # Now we can do an optimizer step\n",
        "          model.zero_grad()                           # Reset gradients tensors  \n",
        "\n",
        "          # print(f'Loss on step #{step}: {loss.item()}')\n",
        "          # train_loss_set.append(loss.item())\n",
        "\n",
        "      # Validation\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the validation set\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_loader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          logits = output[1]\n",
        "          loss = output[0]\n",
        "          # logits = output[0]\n",
        "      \n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "      predictions.append(logits)\n",
        "      true_labels.append(label_ids)\n",
        "\n",
        "      eval_loss += loss.item()\n",
        "\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      \n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "      print(\"Validation Accuracy: {}\".format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJUnU8cFbs7"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammb38i-AvN-"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U4V8LKPBxbh"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class_names = ['Control', 'Depressed']\n",
        "\n",
        "print(classification_report(flat_true_labels, flat_predictions, target_names=class_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyh6Bp_LCoNa"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksNYWH6eCwAZ"
      },
      "source": [
        "df_test = pd.read_csv('twitter_test(2).csv', sep='\\t', encoding='unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5mS9F_AC7ye"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df_test = shuffle(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOKDasG8Wj88"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjzT6Q_yW4--"
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLIJU-OuWIeI"
      },
      "source": [
        "sentences = df_test.tweet.values\n",
        "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
        "labels = df_test.label.values\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 512\n",
        "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGZFuQWeXVg7"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uu9rM-JXqnz"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvloDmNwX1Go"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7UvvG5yX1lf"
      },
      "source": [
        "class_names = ['Control', 'Depressed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBhq4FfRX4mk"
      },
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions, target_names=class_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvgR86FQeP4w"
      },
      "source": [
        "# GloVe + CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5bxO5eTeWWH"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Dense, Flatten, Concatenate, Input\n",
        "from keras.layers.merge import concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrkNzJEAe_UV"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agNkWOAXfGEx"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
        "train_df.rename(columns={0: 'id', 1: 'label', 2: 'alpha', 3: 'text'}, inplace = True)\n",
        "train_df\n",
        "\n",
        "test_df = pd.read_csv('twitter_test(2).csv', sep='\\t', encoding='unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaUUqtkpffOI"
      },
      "source": [
        "X = train_df['text']\n",
        "Y = train_df['label']\n",
        "\n",
        "X_test = test_df['tweet']\n",
        "Y_test = test_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34WSROQy5T0U"
      },
      "source": [
        "sum(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDiALa3mfgmG"
      },
      "source": [
        "vocab_size = 18124\n",
        "oov_token = \"<OOV>\"\n",
        "max_length = 2000\n",
        "padding_type = \"post\"\n",
        "trunction_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gmA6YEqfjOI"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZMJKgPflNr"
      },
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(X)\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
        "                         truncating=trunction_type)\n",
        "\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=padding_type,\n",
        "                        truncating=trunction_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0yqNd5jqYI6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVkU1yHGfnJ3"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('/content/gdrive/My Drive/Thesis Models/GloVe/glove.6B.100d.txt', encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3MKEXHrfvu-"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-RAhmlof1KF"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdyCpOiqz_MB"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "num_epochs = 4\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "for train, test in kfold.split(X_train_padded, Y):\n",
        "  # print(f'Fold Number: {fold_num}')\n",
        "  # fold_num = fold_num+1\n",
        "\t# Fit the model\n",
        "  inputs1 = Input(shape=(length,))\n",
        "  embedding1 = embedding_layer(inputs1)\n",
        "  drop1 = Dropout(0.2)(embedding1)\n",
        "  conv1 = Conv1D(filters=128, kernel_size=3, activation='relu')(drop1)\n",
        "  pool1 = MaxPooling1D(pool_size=3)(conv1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "  # channel 2\n",
        "  conv2 = Conv1D(filters=128, kernel_size=4, activation='relu')(drop1)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "  # channel 3\n",
        "  conv3 = Conv1D(filters=128, kernel_size=5, activation='relu')(drop1)\n",
        "  pool3 = MaxPooling1D(pool_size=5)(conv3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "  # merge\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "  # interpretation\n",
        "  dense1 = Dense(250, activation='relu')(merged)\n",
        "  drop4 = Dropout(0.2)(dense1)\n",
        "  outputs = Dense(1, activation='sigmoid')(drop4)\n",
        "  model = Model(inputs=inputs1, outputs=outputs)\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  history = model.fit(X_train_padded[train], Y[train], epochs=num_epochs, verbose=1)\n",
        "\t# evaluate the model\n",
        "  scores = model.evaluate(X_train_padded[test], Y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "  fold_no = fold_no + 1\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOo3G8OfvbHS"
      },
      "source": [
        "yhat_probs = model.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "# yhat_classes = model.predict_classes(X_test, verbose=0)\n",
        "yhat_classes = yhat_probs.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmNqKomtvjNt"
      },
      "source": [
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[0:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBzGbB-vncH"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "class_names = ['Control', 'Depressed']\n",
        "\n",
        "print(classification_report(y_test, (yhat_probs > 0.5).astype(\"int32\"), target_names=class_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cILN7wRw0fN9"
      },
      "source": [
        "score = model.evaluate([X_test_padded, X_test_padded, X_test_padded], Y_test, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNMUbn900jna"
      },
      "source": [
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict([X_test_padded, X_test_padded, X_test_padded])[0]\n",
        "# predict crisp classes for test set\n",
        "# yhat_classes = model.predict_classes([X_test_padded, X_test_padded, X_test_padded], verbose=0)\n",
        "yhat_classes = np.argmax(yhat_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7PNx4ce0nzs"
      },
      "source": [
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        "# yhat_classes = yhat_classes[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBLjbOlaETzm"
      },
      "source": [
        "yhat_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxsqkhTK0r5q"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Normal', 'Depressed']\n",
        "print(classification_report(Y_test, yhat_classes, target_names=target_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKAj9uSz0wWg"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3I3XUkCbRnc"
      },
      "source": [
        "#GloVe + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDzsJZDZscSM"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE4NF2gkiPf6"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqv1cLW1iX4t"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
        "train_df.rename(columns={0: 'id', 1: 'label', 2: 'alpha', 3: 'text'}, inplace = True)\n",
        "train_df\n",
        "\n",
        "test_df = pd.read_csv('twitter_test(2).csv', sep='\\t', encoding='unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vZa_59GjOHV"
      },
      "source": [
        "X = train_df['text']\n",
        "Y = train_df['label']\n",
        "\n",
        "X_test = test_df['tweet']\n",
        "Y_test = test_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGOCj7hEkCmP"
      },
      "source": [
        "vocab_size = 40000\n",
        "oov_token = \"<OOV>\"\n",
        "max_length = 2000\n",
        "padding_type = \"post\"\n",
        "trunction_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYi2OaWrkF23"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVhvVFnFks8b"
      },
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(X)\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
        "                         truncating=trunction_type)\n",
        "\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=padding_type,\n",
        "                        truncating=trunction_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gv9Bhx7bZoM"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBEn2_u9lzG8"
      },
      "source": [
        "embeddings_index['depression']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vufv6f8rl28k"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WwbWIxfl6c3"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_LleNEyl8qt"
      },
      "source": [
        "# embedding_dim = 16\n",
        "# input_length = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MayoQhF1l_hx"
      },
      "source": [
        "model = Sequential([\n",
        "  # Input(shape=(max_length,), dtype='int32'),\n",
        "    embedding_layer,\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z4ujiWldW36"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "num_epochs = 20\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "cvscores = []\n",
        "fold_num = 0\n",
        "for train, test in kfold.split(X_train_padded, Y):\n",
        "  # print(f'Fold Number: {fold_num}')\n",
        "  # fold_num = fold_num+1\n",
        "\t# Fit the model\n",
        "\thistory = model.fit(X_train_padded[train], Y[train], epochs=num_epochs, verbose=1)\n",
        "\t# evaluate the model\n",
        "\tscores = model.evaluate(X_train_padded[test], Y[test], verbose=1)\n",
        "\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\tcvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qRQiio40H1U"
      },
      "source": [
        "yhat_probs = model.predict(X_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1mCdphoz_Sq"
      },
      "source": [
        "print(classification_report(y_test, (yhat_probs > 0.5).astype(\"int32\"), target_names=class_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyjGrGdt4Rtl"
      },
      "source": [
        "score = model.evaluate(X_test_padded, Y_test, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLnAICNkm88g"
      },
      "source": [
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict(X_test_padded, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "yhat_classes = model.predict_classes(X_test_padded, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlZjVjl1nEPN"
      },
      "source": [
        "# reduce to 1d array\n",
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0AnuJC0BTJ8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Normal', 'Depressed']\n",
        "print(classification_report(Y_test, Yhat_classes, target_names=target_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1j6rPbQcXHi"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYnoads5tg7k"
      },
      "source": [
        "# Sent2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC4I_i9gtjBU"
      },
      "source": [
        "!pip install sent2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DECVBY8tk3v"
      },
      "source": [
        "from scipy import spatial\n",
        "from sent2vec.vectorizer import Vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IGN5SC7tvWE"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ZsH-XFt3Cb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_summed(6).tsv', sep='\\t', header=None)\n",
        "summed_df = pd.read_csv('train_summed(5).tsv', sep='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcvXosZRuMLL"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRNqriSjxjS2"
      },
      "source": [
        "summed_df.dropna()\n",
        "df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKzd7lJouQTM"
      },
      "source": [
        "summed_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_5yVHZEuSOd"
      },
      "source": [
        "tweets = df[3].values\n",
        "summarize = summed_df[3].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spuCD68eur5W"
      },
      "source": [
        "print(f'Sentences: {df.shape} ++++ Summarize: {summed_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMopN30L0Yds"
      },
      "source": [
        "tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5wHUNDFujuB"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "total_dist = 0\n",
        "total_summarized = 0\n",
        "summed6_vectors = []\n",
        "summed5_vectors = []\n",
        "\n",
        "for sent in summarize:\n",
        "  try:\n",
        "    index = np.where(summarize == sent)[0][0]\n",
        "    if sent != tweets[index]:\n",
        "      sentences = [\n",
        "                  sent,\n",
        "                  tweets[index]\n",
        "      ]\n",
        "      vectorizer.bert(sentences)\n",
        "      vectors_bert = vectorizer.vectors\n",
        "      summed6_vectors.append(vectors_bert[0])\n",
        "      summed5_vectors.append(vectors_bert[1])\n",
        "\n",
        "      dist = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])\n",
        "      if dist != 0:\n",
        "        total_dist+=dist\n",
        "        total_summarized+=1\n",
        "      print(f'Cosine dist {index}: {dist}')\n",
        "      # print(f'Data number: {index}')\n",
        "  except:\n",
        "    print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtmukpBdviRT"
      },
      "source": [
        "avg_cosine_dist = total_dist/total_summarized\n",
        "print(f'Average cosine distance: {avg_cosine_dist}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM7XlVQamCrl"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-zTeg4ZZuSN"
      },
      "source": [
        "np.save('newer_summed(5)_vectors.npy', summed5_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dja_SG7Do_f"
      },
      "source": [
        "vector_5 = np.load('newer_summed(5)_vectors.npy')\n",
        "vector_6 = np.load('new_summed(6)_vectors.npy')\n",
        "\n",
        "comparison = vector_5 == vector_6\n",
        "# comparison = summed5_vectors == summed6_vectors\n",
        "equal_arrays = comparison.all()\n",
        "\n",
        "print(equal_arrays)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2IHoW82xjWO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# sent_vectors = np.load('sent_vectors.npy')\n",
        "summed_vectors = np.load('summed(9)_vectors.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW2ZxrgmZQrg"
      },
      "source": [
        "# sentVec_df = pd.DataFrame(sent_vectors)\n",
        "summedVec_df = pd.DataFrame(summed_vectors)\n",
        "# sentences_df = pd.DataFrame(sentences_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ROLN0mXZ0be"
      },
      "source": [
        "# print(sentVec_df.shape)\n",
        "print(summedVec_df.shape)\n",
        "# print(sentences_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWRJJv3ZiukB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standardized_data = StandardScaler().fit_transform(summedVec_df)\n",
        "print(standardized_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUyUWVDb5OP"
      },
      "source": [
        "sample_data = standardized_data\n",
        "\n",
        "covar_matrix = (np.matmul(sample_data.T, sample_data))/1577\n",
        "print(f'The shape of our covar matrix is: {covar_matrix.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maKmkR_QcNzq"
      },
      "source": [
        "from scipy.linalg import eigh\n",
        "\n",
        "values, vectors = eigh(covar_matrix, eigvals=(766,767))\n",
        "print(values.shape)\n",
        "print(values)\n",
        "print(f\"Shape of eigen vectors: {vectors.shape}\")\n",
        "\n",
        "vectors = vectors.T\n",
        "print(f'Updated shape of eigen vectors: {vectors.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP7kZPzyfZph"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "new_coordinates = np.matmul(vectors, sample_data.T)\n",
        "\n",
        "print(f'Resultantat new data points\\'s shape {vectors.shape}, X {sample_data.T.shape} = {new_coordinates.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jpxwjBZgz_I"
      },
      "source": [
        "labels = pd.DataFrame(index=range(1), columns=range(1577))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KItaS3Dthek8"
      },
      "source": [
        "labels.fillna(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcjouJ8ud9Ol"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "new_coordinates = np.vstack((new_coordinates, labels)).T\n",
        "\n",
        "dataframe_6 = pd.DataFrame(data = new_coordinates, columns=('1st Principal', '2nd Principal', 'label'))\n",
        "dataframe_6.label = 9\n",
        "print(dataframe_2.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hk-pQp7jrZj"
      },
      "source": [
        "import seaborn as sn\n",
        "sn.FacetGrid(dataframe, hue='label', size=6).map(plt.scatter, '1st Principal', '2nd Principal').add_legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRwg8jBlnCtW"
      },
      "source": [
        "pca_dataframe = dataframe.drop(columns='label')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, whiten=True).fit(pca_dataframe)\n",
        "X_pca = pca.transform(pca_dataframe)\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=1).fit(X_pca)\n",
        "\n",
        "\n",
        "centers = pca.inverse_transform(kmeans.cluster_centers_)\n",
        "print(centers)\n",
        "\n",
        "plt.scatter(centers[0][0], centers[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qw1drT3ubNB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(centers[:,0], centers[:,1])\n",
        "n = ['Vanilla', 'Summed(2)', 'Summed(5)', 'Summed(6)', 'Summed(8)', 'Summed(9)']\n",
        "n = ['Original', '50%', '40%', '30%', '10%', '90%']\n",
        "\n",
        "for i, txt in enumerate(n):\n",
        "    ax.annotate(txt, (centers[:,0][i], centers[:,1][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PKYEUenJ_fT"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRY1mQ2IlVbi"
      },
      "source": [
        "centroids = np.load('centroids_vectors.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzamJqTw4p5D"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "ratio_1 = np.load('sent_vectors.npy')\n",
        "ratio_2 = np.load('summed(2)_vectors.npy')\n",
        "ratio_5 = np.load('new_summed(5)_vectors.npy')\n",
        "ratio_6 = np.load('new_summed(6)_vectors.npy')\n",
        "ratio_8 = np.load('summed(8)_vectors.npy')\n",
        "ratio_9 = np.load('summed(9)_vectors.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DA8DMUD5gbw"
      },
      "source": [
        "x1 = dataframe_1['1st Principal']\n",
        "y1 = dataframe_1['2nd Principal']\n",
        "\n",
        "x2 = dataframe_2['1st Principal']\n",
        "y2 = dataframe_2['2nd Principal']\n",
        "\n",
        "x3 = dataframe_3['1st Principal']\n",
        "y3 = dataframe_3['2nd Principal']\n",
        "\n",
        "x4 = dataframe_4['1st Principal']\n",
        "y4 = dataframe_4['2nd Principal']\n",
        "\n",
        "x5 = dataframe_5['1st Principal']\n",
        "y5 = dataframe_5['2nd Principal']\n",
        "\n",
        "x6 = dataframe_6['1st Principal']\n",
        "y6 = dataframe_6['2nd Principal']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkYCqCGg5H4K"
      },
      "source": [
        "vectors = np.vstack((dataframe_1, dataframe_2, dataframe_3, dataframe_4, dataframe_5, dataframe_6))\n",
        "n = ['Vanilla', 'Summed(2)', 'Summed(5)', 'Summed(6)', 'Summed(8)', 'Summed(9)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2iDe7PK49Qk"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,16))\n",
        "\n",
        "ratio_1 = ax.scatter(x1, y1, color='#1f77b4')\n",
        "ratio_2 = ax.scatter(x2, y2, color='#ff7f0e')\n",
        "ratio_3 = ax.scatter(x3, y3, color='#2ca02c')\n",
        "ratio_4 = ax.scatter(x4, y4, color='#d62728')\n",
        "ratio_5 = ax.scatter(x5, y5, color='#9467bd')\n",
        "ratio_6 = ax.scatter(x6, y6, color='#8c564b')\n",
        "ax.set_xlabel('1st Principal')\n",
        "ax.set_ylabel('2nd Principal')\n",
        "ax.set_title('scatter plot')\n",
        "\n",
        "plt.legend((ratio_1, ratio_2, ratio_3, ratio_4, ratio_5, ratio_6),\n",
        "           ('Vanilla', '50%', '40%', '30%', '10%', '90%'),\n",
        "           scatterpoints=1,\n",
        "           loc='lower left',\n",
        "           ncol=3,\n",
        "           fontsize=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38woj-FzCC5J"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "centroid_1 = np.load('centroid_sent.npy')\n",
        "centroid_2 = np.load('centroid_summed(2).npy')\n",
        "centroid_5 = np.load('centroid_summed(5).npy')\n",
        "centroid_6 = np.load('centroid_summed(6).npy')\n",
        "centroid_8 = np.load('centroid_summed(8).npy')\n",
        "centroid_9 = np.load('centroid_summed(9).npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPu9lFUKPCKn"
      },
      "source": [
        "n = ['Vanilla', 'Summed(2)', 'Summed(5)', 'Summed(6)', 'Summed(8)', 'Summed(9)']\n",
        "centers = np.vstack((centroid_1, centroid_2, centroid_5, centroid_6, centroid_8, centroid_9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uht7ti_sCidV"
      },
      "source": [
        "centroid_9 == centroid_8"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}